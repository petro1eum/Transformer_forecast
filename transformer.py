import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import math
import os
import time
from typing import Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è M1 Max
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'


class PositionalEncoding(nn.Module):
    """–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]


class MultiHeadAttention(nn.Module):
    """Multi-Head Attention –º–µ—Ö–∞–Ω–∏–∑–º"""
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % n_heads == 0
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
        
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, 
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = query.size()
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –≥–æ–ª–æ–≤—ã
        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = torch.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ attention –∫ values
        context = torch.matmul(attention_weights, V)
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≥–æ–ª–æ–≤
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        output = self.W_o(context)
        return output


class FeedForward(nn.Module):
    """Feed Forward —Å–µ—Ç—å"""
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear2(self.dropout(self.relu(self.linear1(x))))


class EncoderLayer(nn.Module):
    """–°–ª–æ–π —ç–Ω–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self-attention —Å residual connection –∏ layer norm
        attn_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feed forward —Å residual connection –∏ layer norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x


class DecoderLayer(nn.Module):
    """–°–ª–æ–π –¥–µ–∫–æ–¥–µ—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞"""
    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor, enc_output: torch.Tensor, 
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Masked self-attention
        attn_output = self.self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # Cross-attention
        attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))
        
        # Feed forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x


class TimeSeriesTransformer(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    def __init__(self, 
                 input_dim: int,
                 d_model: int = 512,
                 n_heads: int = 8,
                 n_encoder_layers: int = 6,
                 n_decoder_layers: int = 6,
                 d_ff: int = 2048,
                 max_seq_length: int = 1000,
                 dropout: float = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        self.input_embedding = nn.Linear(input_dim, d_model)
        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)
        
        # Learnable time embeddings –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        self.time_embedding = nn.Parameter(torch.randn(1, max_seq_length, d_model) * 0.02)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        self.value_embedding = nn.Linear(input_dim, d_model // 4)
        self.trend_embedding = nn.Linear(input_dim, d_model // 4)
        self.seasonal_embedding = nn.Linear(input_dim, d_model // 4)
        self.residual_embedding = nn.Linear(input_dim, d_model // 4)
        
        # Encoder
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_encoder_layers)
        ])
        
        # Decoder
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(n_decoder_layers)
        ])
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º
        self.final_norm = nn.LayerNorm(d_model)
        self.output_projection = nn.Linear(d_model, input_dim)
        self.dropout = nn.Dropout(dropout)
        
    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask
    
    def forward(self, src: torch.Tensor, tgt: torch.Tensor, 
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ embedding —Å –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–µ–π
        batch_size, seq_len, _ = src.shape
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ embedding
        src_emb = self.input_embedding(src) * math.sqrt(self.d_model)
        
        # –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        value_emb = self.value_embedding(src)
        trend_emb = self.trend_embedding(src)
        seasonal_emb = self.seasonal_embedding(src)
        residual_emb = self.residual_embedding(src)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        decomposed_emb = torch.cat([value_emb, trend_emb, seasonal_emb, residual_emb], dim=-1)
        src = src_emb + decomposed_emb
        
        src = self.positional_encoding(src)
        
        # –î–æ–±–∞–≤–ª—è–µ–º learnable time embeddings
        if seq_len <= self.max_seq_length:
            src = src + self.time_embedding[:, :seq_len, :]
        
        src = self.dropout(src)
        
        # –ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ encoder
        enc_output = src
        for encoder_layer in self.encoder_layers:
            enc_output = encoder_layer(enc_output, src_mask)
        
        # Embedding –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        tgt = self.input_embedding(tgt) * math.sqrt(self.d_model)
        tgt = self.positional_encoding(tgt)
        
        # –î–æ–±–∞–≤–ª—è–µ–º learnable time embeddings –¥–ª—è target
        tgt_len = tgt.size(1)
        if tgt_len <= self.max_seq_length:
            tgt = tgt + self.time_embedding[:, :tgt_len, :]
        
        tgt = self.dropout(tgt)
        
        # –ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ decoder
        dec_output = tgt
        for decoder_layer in self.decoder_layers:
            dec_output = decoder_layer(dec_output, enc_output, src_mask, tgt_mask)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–æ–µ–∫—Ü–∏—è
        dec_output = self.final_norm(dec_output)
        output = self.output_projection(dec_output)
        
        return output


class TimeSeriesDataset(Dataset):
    """Dataset –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π"""
    def __init__(self, data: np.ndarray, seq_length: int, pred_length: int, augment: bool = True):
        self.data = torch.FloatTensor(data)
        self.seq_length = seq_length
        self.pred_length = pred_length
        self.augment = augment
        
    def __len__(self):
        return len(self.data) - self.seq_length - self.pred_length + 1
    
    def __getitem__(self, idx):
        src = self.data[idx:idx + self.seq_length].clone()
        tgt = self.data[idx + self.seq_length:idx + self.seq_length + self.pred_length].clone()
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if self.augment and torch.rand(1) < 0.3:  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            noise_std = 0.02 * src.std()
            src += torch.randn_like(src) * noise_std
            tgt += torch.randn_like(tgt) * noise_std
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            if torch.rand(1) < 0.5:
                scale = 0.95 + torch.rand(1) * 0.1  # 0.95-1.05
                src *= scale
                tgt *= scale
        
        return src, tgt


def create_synthetic_data(n_samples: int = 1000) -> np.ndarray:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    np.random.seed(42)  # –§–∏–∫—Å–∏—Ä—É–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    time = np.arange(n_samples)
    # –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç—Ä–µ–Ω–¥–∞, —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –∏ —à—É–º–∞
    trend = 0.02 * time  # –£–º–µ–Ω—å—à–∞–µ–º —Ç—Ä–µ–Ω–¥
    seasonal1 = 8 * np.sin(2 * np.pi * time / 50)  # –û—Å–Ω–æ–≤–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
    seasonal2 = 3 * np.sin(2 * np.pi * time / 100)  # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
    seasonal3 = 2 * np.sin(2 * np.pi * time / 25)   # –ö–æ—Ä–æ—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
    noise = np.random.normal(0, 1.5, n_samples)  # –£–º–µ–Ω—å—à–∞–µ–º —à—É–º
    data = trend + seasonal1 + seasonal2 + seasonal3 + noise + 20  # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
    return data.reshape(-1, 1)


def trend_aware_loss(predictions: torch.Tensor, targets: torch.Tensor, alpha: float = 0.3) -> torch.Tensor:
    """–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å —É—á–µ—Ç–æ–º —Ç—Ä–µ–Ω–¥–∞"""
    mse_loss = nn.MSELoss()(predictions, targets)
    
    # –ü–æ—Ç–µ—Ä–∏ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π (—Ç—Ä–µ–Ω–¥)
    pred_diff = predictions[:, 1:] - predictions[:, :-1]
    target_diff = targets[:, 1:] - targets[:, :-1]
    trend_loss = nn.MSELoss()(pred_diff, target_diff)
    
    return mse_loss + alpha * trend_loss

def train_model(model: nn.Module, 
                train_loader: DataLoader, 
                val_loader: DataLoader,
                n_epochs: int = 100,
                learning_rate: float = 0.001,
                device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏ –∏ early stopping"""
    model = model.to(device)
    criterion = nn.MSELoss()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–æ—Å—Ç–æ–π MSE
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    train_losses = []
    val_losses = []
    
    # Early stopping
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 10
    best_model_state = None
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(train_loader)} –±–∞—Ç—á–∞—Ö...")
    print(f"   üõë Early stopping: patience={patience}")
    training_start_time = time.time()
    
    for epoch in range(n_epochs):
        epoch_start_time = time.time()
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0
        train_start_time = time.time()
        
        for batch_idx, (src, tgt) in enumerate(train_loader):
            src, tgt = src.to(device), tgt.to(device)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ö–æ–¥–∞ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞ (—Å–¥–≤–∏–Ω—É—Ç–∞—è —Ü–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            tgt_input = torch.cat([src[:, -1:, :], tgt[:, :-1, :]], dim=1)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞
            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)
            
            # Forward pass
            optimizer.zero_grad()
            output = model(src, tgt_input, tgt_mask=tgt_mask)
            loss = criterion(output, tgt)
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        train_time = time.time() - train_start_time
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_loss = 0
        val_start_time = time.time()
        
        with torch.no_grad():
            for src, tgt in val_loader:
                src, tgt = src.to(device), tgt.to(device)
                tgt_input = torch.cat([src[:, -1:, :], tgt[:, :-1, :]], dim=1)
                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)
                output = model(src, tgt_input, tgt_mask=tgt_mask)
                loss = criterion(output, tgt)
                val_loss += loss.item()
        
        val_time = time.time() - val_start_time
        epoch_time = time.time() - epoch_start_time
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        
        scheduler.step(avg_val_loss)
        
        # Early stopping logic
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
        
        if epoch % 5 == 0:  # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö –≤–º–µ—Å—Ç–æ 10
            print(f'Epoch {epoch:3d}/{n_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print(f'   ‚è±Ô∏è  Train: {train_time:.2f}s | Val: {val_time:.2f}s | Total: {epoch_time:.2f}s | Speed: {len(train_loader)/train_time:.1f} batch/s')
            print(f'   üéØ Best Val Loss: {best_val_loss:.4f} | Patience: {patience_counter}/{patience}')
        
        # Early stopping check
        if patience_counter >= patience:
            print(f'\nüõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch}! Val loss –Ω–µ —É–ª—É—á—à–∞–ª—Å—è {patience} —ç–ø–æ—Ö.')
            break
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å Val Loss: {best_val_loss:.6f}")
    
    total_training_time = time.time() - training_start_time
    print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_training_time:.1f}s ({total_training_time/60:.1f} –º–∏–Ω)")
    print(f"   –í—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É: {total_training_time/len(train_losses):.1f}s")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {train_losses[-1]:.6f}")
    print(f"   –õ—É—á—à–∏–π Val Loss: {best_val_loss:.6f}")
    
    return train_losses, val_losses


def forecast(model: nn.Module, 
             src_sequence: torch.Tensor, 
             n_steps: int,
             device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
             temperature: float = 1.0) -> torch.Tensor:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π –∏ –∞–Ω—Å–∞–º–±–ª–µ–º"""
    model.eval()
    src_sequence = src_sequence.to(device)
    
    predictions = []
    
    # –ê–Ω—Å–∞–º–±–ª—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ —Å —Ä–∞–∑–Ω—ã–º dropout
    for ensemble_idx in range(3):
        model.train() if ensemble_idx > 0 else model.eval()  # –í–∫–ª—é—á–∞–µ–º dropout –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        
        with torch.no_grad():
            # –ù–∞—á–∞–ª—å–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –¥–µ–∫–æ–¥–µ—Ä–∞
            dec_input = src_sequence[:, -1:, :]
            
            for step in range(n_steps):
                tgt_mask = model.generate_square_subsequent_mask(dec_input.size(1)).to(device)
                output = model(src_sequence, dec_input, tgt_mask=tgt_mask)
                next_pred = output[:, -1:, :]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                if temperature != 1.0 and ensemble_idx > 0:
                    next_pred = next_pred / temperature
                
                dec_input = torch.cat([dec_input, next_pred], dim=1)
        
        predictions.append(dec_input[:, 1:, :])
    
    # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª—è
    ensemble_pred = torch.stack(predictions).mean(dim=0)
    return ensemble_pred


def visualize_results(train_data: np.ndarray, 
                     test_data: np.ndarray,
                     predictions: np.ndarray,
                     train_losses: list,
                     val_losses: list):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤
    axes[0].plot(range(len(train_data)), train_data, label='Train Data', alpha=0.7)
    axes[0].plot(range(len(train_data), len(train_data) + len(test_data)), 
                test_data, label='Actual Test Data', alpha=0.7)
    axes[0].plot(range(len(train_data), len(train_data) + len(predictions)), 
                predictions, label='Predictions', linestyle='--', alpha=0.8)
    axes[0].set_xlabel('Time')
    axes[0].set_ylabel('Value')
    axes[0].set_title('Time Series Forecasting with Transformer')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
    axes[1].plot(train_losses, label='Train Loss')
    axes[1].plot(val_losses, label='Validation Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('MSE Loss')
    axes[1].set_title('Training Progress')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–º–µ—Ä–∞"""
    script_start_time = time.time()
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    seq_length = 60      # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–∫–Ω–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
    pred_length = 15     # –£–º–µ–Ω—å—à–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞
    batch_size = 32      # –£–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –ª—É—á—à–µ–π –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏–∏
    n_epochs = 80        # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    data_start_time = time.time()
    data = create_synthetic_data(2000)
    print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {time.time() - data_start_time:.2f}s")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    train_size = int(0.8 * len(data))
    train_data = data[:train_size]
    test_data = data[train_size:]
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    mean = train_data.mean()
    std = train_data.std()
    train_data_norm = (train_data - mean) / std
    test_data_norm = (test_data - mean) / std
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
    train_dataset = TimeSeriesDataset(train_data_norm[:-100], seq_length, pred_length, augment=False)
    val_dataset = TimeSeriesDataset(train_data_norm[-100-seq_length-pred_length:], seq_length, pred_length, augment=False)
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ DataLoader –¥–ª—è M1 Max (10 —è–¥–µ—Ä)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=6, pin_memory=True, persistent_workers=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size,
                           num_workers=4, pin_memory=True, persistent_workers=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è M1/M2 Mac)
    if torch.backends.mps.is_available():
        device = 'mps'
        print("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º Metal Performance Shaders (MPS) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è!")
        print(f"   MPS built: {torch.backends.mps.is_built()}")
        # –û—á–∏—â–∞–µ–º –∫—ç—à MPS –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
    elif torch.cuda.is_available():
        device = 'cuda'
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è!")
    else:
        device = 'cpu'
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞...")
    model_start_time = time.time()
    model = TimeSeriesTransformer(
        input_dim=1,
        d_model=128,     # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
        n_heads=8,       # 8 –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        n_encoder_layers=4,  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
        n_decoder_layers=4,  # –°–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ
        d_ff=512,        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º feed-forward –¥–ª—è –ª—É—á—à–µ–π —ç–∫—Å–ø—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç–∏
        max_seq_length=200,
        dropout=0.2      # –£–º–µ—Ä–µ–Ω–Ω—ã–π dropout
    )
    
    model_creation_time = time.time() - model_start_time
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∑–∞ {model_creation_time:.2f}s")
    print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   üìà Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}")
    print(f"   ‚öôÔ∏è  Batch size: {batch_size} | Epochs: {n_epochs}")
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    train_losses, val_losses = train_model(model, train_loader, val_loader, n_epochs=n_epochs, learning_rate=0.0005, device=device)
    
    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
    print(f"   üìè –î–ª–∏–Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_data_norm)}")
    prediction_start_time = time.time()
    model.eval()
    test_predictions = []
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    prediction_steps = 0
    for i in range(0, len(test_data_norm) - seq_length, pred_length):
        src = torch.FloatTensor(test_data_norm[i:i+seq_length]).unsqueeze(0)
        pred = forecast(model, src, pred_length, device, temperature=1.0)  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
        test_predictions.extend(pred.squeeze().cpu().numpy())
        prediction_steps += 1
        
        if prediction_steps % 5 == 0:
            print(f"   üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–æ {prediction_steps} —à–∞–≥–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        if i + seq_length + pred_length >= len(test_data_norm):
            break
    
    prediction_time = time.time() - prediction_start_time
    print(f"   ‚úÖ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {prediction_time:.2f}s")
    print(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {prediction_steps} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(test_predictions)}")
    print(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {prediction_steps/prediction_time:.1f} —à–∞–≥–æ–≤/—Å–µ–∫")
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    test_predictions = np.array(test_predictions) * std + mean
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    viz_start_time = time.time()
    visualize_results(
        train_data.squeeze(), 
        test_data[:len(test_predictions)].squeeze(),
        test_predictions.squeeze(),
        train_losses,
        val_losses
    )
    viz_time = time.time() - viz_start_time
    print(f"   ‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {viz_time:.2f}s")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    actual = test_data[:len(test_predictions)].squeeze()
    predicted = test_predictions.squeeze()
    mse = np.mean((actual - predicted) ** 2)
    mae = np.mean(np.abs(actual - predicted))
    
    print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   MSE: {mse:.4f}")
    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {np.sqrt(mse):.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_start_time = time.time()
    torch.save({
        'model_state_dict': model.state_dict(),
        'mean': mean,
        'std': std,
        'model_config': {
            'input_dim': 1,
            'd_model': 128,
            'n_heads': 8,
            'n_encoder_layers': 3,
            'n_decoder_layers': 3,
            'd_ff': 256,
            'max_seq_length': 200,
            'dropout': 0.25
        }
    }, 'transformer_timeseries_model.pth')
    save_time = time.time() - save_start_time
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∑–∞ {save_time:.2f}s –≤ 'transformer_timeseries_model.pth'")
    
    # –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    total_script_time = time.time() - script_start_time
    print(f"\nüéØ –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {total_script_time:.1f}s ({total_script_time/60:.1f} –º–∏–Ω)")
    print(f"   üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {data_start_time:.2f}s")
    print(f"   üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_creation_time:.2f}s") 
    print(f"   üöÄ –û–±—É—á–µ–Ω–∏–µ: {total_script_time - prediction_start_time + prediction_time:.1f}s")
    print(f"   üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ: {prediction_time:.2f}s")
    print(f"   üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: {viz_time:.2f}s")


if __name__ == "__main__":
    main()