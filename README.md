# Transformer_forecast

**LLM-Optimized Time Series Transformer**

---

## üìà –û–ø–∏—Å–∞–Ω–∏–µ

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –±–∞–∑–µ LLM (GPT-4, Ollama –∏ –¥—Ä.). –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é LLM (function calling, structured output), –∞ —Ç–∞–∫–∂–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–¥ Apple Silicon (M1/M2/M3, MPS).

---

## üöÄ –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **Transformer –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤** (PyTorch, encoder-decoder)
- **LLM-–≥–∏–ø–µ—Ä–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ rule-based, OpenAI GPT-4, Ollama (Qwen2.5)
- **Structured output**: —á–∏—Å—Ç—ã–π function calling –±–µ–∑ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä learning rate, dropout, gradient clipping, early stopping**
- **MPS-—É—Å–∫–æ—Ä–µ–Ω–∏–µ** –¥–ª—è MacBook (Apple Silicon)
- **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ LLM-—Ä–µ—à–µ–Ω–∏–π**
- **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤**

---

## üõ†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∑–∞–ø—É—Å–∫

1. **–ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:**
   ```bash
   git clone https://github.com/yourname/Transformer_forecast.git
   cd Transformer_forecast
   ```
2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:**
   ```bash
   pip install -r requirements.txt
   ```
3. **–°–æ–∑–¥–∞–π—Ç–µ .env —Ñ–∞–π–ª:**
   ```bash
   cp .env.example .env
   # –î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–π OPENAI_API_KEY
   ```
4. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:**
   ```bash
   python LLM_transformer.py
   ```

---

## ‚öôÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

- `LLM_transformer.py` ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç —Å LLM-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
- `transformer.py` ‚Äî –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã
- `requirements.txt` ‚Äî –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- `.gitignore` ‚Äî –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, –º–æ–¥–µ–ª–µ–π, —Å–µ–∫—Ä–µ—Ç–æ–≤
- `llm_transformer_model.pth` ‚Äî —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≥–∏—Ç–æ–º)
- `llm_optimization_results.json` ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –≥–∏—Ç–æ–º)

---

## üìä –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞

```
[LLM] epoch=9 action=adjust_lr params={'factor': 0.5, 'lr_factor': 0.5} val_loss=0.0453
üîß LLM: –£–º–µ–Ω—å—à–µ–Ω learning rate –Ω–∞ 0.50x ‚Üì (–Ω–æ–≤—ã–π LR: 0.000500)
[LLM] epoch=15 action=early_stop params={} val_loss=0.0338
üõë LLM: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç early stopping
‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å Val Loss: 0.029483
```

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Ollama (Qwen2.5)](https://ollama.com/library/qwen2.5)
- [Time Series Transformer (original paper)](https://arxiv.org/abs/2010.02803)

---

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License 