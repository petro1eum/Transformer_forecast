import torch
import torch.nn as nn
import time
from transformer import TimeSeriesTransformer

def benchmark_transformer(device_name, batch_size=32):
    """Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ½Ğ° Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°Ñ…"""
    device = torch.device(device_name)
    print(f"\nğŸ”¥ Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€ Ğ½Ğ° {device_name.upper()}...")
    print(f"   Batch size: {batch_size}")
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
    model = TimeSeriesTransformer(
        input_dim=1,
        d_model=128,
        n_heads=8,
        n_encoder_layers=3,
        n_decoder_layers=3,
        d_ff=256,
        max_seq_length=200,
        dropout=0.15
    ).to(device)
    
    # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    seq_len = 60
    pred_len = 15
    src = torch.randn(batch_size, seq_len, 1).to(device)
    tgt = torch.randn(batch_size, pred_len, 1).to(device)
    
    # ĞŸÑ€Ğ¾Ğ³Ñ€ĞµĞ²
    model.train()
    for _ in range(5):
        tgt_mask = model.generate_square_subsequent_mask(pred_len).to(device)
        _ = model(src, tgt, tgt_mask=tgt_mask)
    
    # Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº
    start_time = time.time()
    iterations = 100
    
    for _ in range(iterations):
        tgt_mask = model.generate_square_subsequent_mask(pred_len).to(device)
        output = model(src, tgt, tgt_mask=tgt_mask)
        loss = output.sum()
        loss.backward()
        model.zero_grad()
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"â±ï¸  Ğ’Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ {iterations} Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹: {total_time:.3f} ÑĞµĞº")
    print(f"ğŸš€ Ğ¡ĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ: {iterations/total_time:.1f} Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹/ÑĞµĞº")
    print(f"ğŸ“Š Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ: {total_time/iterations*1000:.1f} Ğ¼Ñ")
    
    return total_time

def main():
    print("ğŸ§ª Ğ‘ĞµĞ½Ñ‡Ğ¼Ğ°Ñ€Ğº Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ĞµÑ€Ğ° Ğ½Ğ° M1 Mac")
    print("=" * 50)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°
    print("ğŸ“± Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ°:")
    print(f"   CPU: âœ…")
    print(f"   MPS: {'âœ…' if torch.backends.mps.is_available() else 'âŒ'}")
    
    batch_sizes = [16, 32, 64]
    
    for batch_size in batch_sizes:
        print(f"\n{'='*20} BATCH SIZE {batch_size} {'='*20}")
        
        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ CPU
        cpu_time = benchmark_transformer('cpu', batch_size)
        
        # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ MPS ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
        if torch.backends.mps.is_available():
            mps_time = benchmark_transformer('mps', batch_size)
            
            speedup = cpu_time / mps_time
            print(f"\nğŸ† Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« Ğ´Ğ»Ñ batch_size={batch_size}:")
            print(f"   CPU Ğ²Ñ€ĞµĞ¼Ñ: {cpu_time:.3f} ÑĞµĞº")
            print(f"   MPS Ğ²Ñ€ĞµĞ¼Ñ: {mps_time:.3f} ÑĞµĞº")
            if speedup > 1:
                print(f"   ğŸš€ MPS Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ² {speedup:.1f}x Ñ€Ğ°Ğ·!")
            else:
                print(f"   ğŸŒ CPU Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ Ğ² {1/speedup:.1f}x Ñ€Ğ°Ğ·")

if __name__ == "__main__":
    main() 