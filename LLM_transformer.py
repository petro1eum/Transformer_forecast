import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import math
import os
import time
import json
import requests
from typing import Tuple, Optional, Dict, List, Any
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
from dotenv import load_dotenv
load_dotenv()

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è M1 Max
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
from transformer import (
    PositionalEncoding, MultiHeadAttention, FeedForward, 
    EncoderLayer, DecoderLayer, TimeSeriesTransformer,
    TimeSeriesDataset, create_synthetic_data, visualize_results, forecast
)

@dataclass
class TrainingMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ LLM"""
    epoch: int
    train_loss: float
    val_loss: float
    learning_rate: float
    loss_improvement: float
    gradient_norm: float
    training_time: float
    overfitting_score: float = 0.0
    convergence_score: float = 0.0
    
    def __post_init__(self):
        # –ù–æ–≤—ã–π —Ä–∞—Å—á–µ—Ç overfitting_score: –∑–Ω–∞–∫ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
        gap = self.train_loss - self.val_loss
        self.overfitting_score = gap / (abs(self.val_loss) + 1e-8)
        self.convergence_score = abs(self.loss_improvement) / max(abs(self.val_loss), 1e-8)

class OptimizationAction(Enum):
    """–í–æ–∑–º–æ–∂–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
    CONTINUE = "continue"
    ADJUST_LR = "adjust_lr"
    ADJUST_DROPOUT = "adjust_dropout"
    EARLY_STOP = "early_stop"
    REDUCE_BATCH_SIZE = "reduce_batch_size"
    INCREASE_BATCH_SIZE = "increase_batch_size"
    ENABLE_GRADIENT_CLIPPING = "enable_gradient_clipping"
    ADJUST_WEIGHT_DECAY = "adjust_weight_decay"

@dataclass
class OptimizationSuggestion:
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –æ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
    action: OptimizationAction
    parameters: Dict[str, Any]
    confidence: float
    reasoning: str
    priority: int = 1

class LLMHyperparameterOptimizer:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å LLM"""
    
    def __init__(self, model_type: str = "rule_based", api_key: Optional[str] = None):
        self.model_type = model_type
        self.api_key = api_key
        self.history: List[TrainingMetrics] = []
        self.suggestions_history: List[OptimizationSuggestion] = []
        self.performance_tracker = {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è rule-based –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.rules_config = {
            'overfitting_threshold': 0.15,
            'stagnation_patience': 5,
            'lr_reduction_factor': 0.5,
            'dropout_increase_step': 0.05,
            'gradient_clip_threshold': 1.0
        }
        
        print(f"üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {model_type}")
    
    def analyze_training_progress(self, metrics: TrainingMetrics) -> OptimizationSuggestion:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
        self.history.append(metrics)
        
        if self.model_type == "rule_based":
            return self._rule_based_analysis(metrics)
        elif self.model_type == "local_llm":
            return self._local_llm_analysis(metrics)
        elif self.model_type == "openai":
            return self._openai_analysis(metrics)
        else:
            return OptimizationSuggestion(
                action=OptimizationAction.CONTINUE,
                parameters={},
                confidence=0.5,
                reasoning="Unknown optimizer type"
            )
    
    def _rule_based_analysis(self, metrics: TrainingMetrics) -> OptimizationSuggestion:
        """Rule-based –∞–Ω–∞–ª–∏–∑ (–±—ã—Å—Ç—Ä—ã–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π)"""
        # 1. –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ë–ê–ì–ê #2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (train_loss < val_loss –æ–∑–Ω–∞—á–∞–µ—Ç overfitting)
        gap = metrics.train_loss - metrics.val_loss
        if gap < 0 and abs(metrics.overfitting_score) > self.rules_config['overfitting_threshold']:
            if len(self.history) >= 3:
                recent = self.history[-3:]
                if all((m.train_loss - m.val_loss) < 0 and abs(m.overfitting_score) > self.rules_config['overfitting_threshold'] for m in recent):
                    return OptimizationSuggestion(
                        action=OptimizationAction.ADJUST_DROPOUT,
                        parameters={'dropout_increase': self.rules_config['dropout_increase_step']},
                        confidence=0.8,
                        reasoning=f"–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (train < val, score: {metrics.overfitting_score:.3f}). –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º dropout.",
                        priority=1
                    )
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏
        if len(self.history) >= self.rules_config['stagnation_patience']:
            recent_improvements = [m.loss_improvement for m in self.history[-self.rules_config['stagnation_patience']:]]
            if all(abs(imp) < 0.001 for imp in recent_improvements):
                return OptimizationSuggestion(
                    action=OptimizationAction.ADJUST_LR,
                    parameters={'lr_factor': self.rules_config['lr_reduction_factor']},
                    confidence=0.7,
                    reasoning=f"–°—Ç–∞–≥–Ω–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è. –£–º–µ–Ω—å—à–∞–µ–º learning rate –≤ {1/self.rules_config['lr_reduction_factor']:.1f} —Ä–∞–∑.",
                    priority=2
                )
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if metrics.gradient_norm > 10.0:
            return OptimizationSuggestion(
                action=OptimizationAction.ENABLE_GRADIENT_CLIPPING,
                parameters={'max_norm': self.rules_config['gradient_clip_threshold']},
                confidence=0.9,
                reasoning=f"–ë–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (norm: {metrics.gradient_norm:.2f}). –í–∫–ª—é—á–∞–µ–º gradient clipping.",
                priority=1
            )
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if len(self.history) >= 10:
            recent_losses = [m.val_loss for m in self.history[-10:]]
            if recent_losses[0] - recent_losses[-1] < 0.001:  # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
                return OptimizationSuggestion(
                    action=OptimizationAction.ADJUST_LR,
                    parameters={'lr_factor': 1.2},  # –ù–µ–±–æ–ª—å—à–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
                    confidence=0.6,
                    reasoning="–ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å. –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º learning rate.",
                    priority=3
                )
        
        # 5. –í—Å–µ —Ö–æ—Ä–æ—à–æ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        return OptimizationSuggestion(
            action=OptimizationAction.CONTINUE,
            parameters={},
            confidence=0.9,
            reasoning="–û–±—É—á–µ–Ω–∏–µ –∏–¥–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.",
            priority=5
        )
    
    def _local_llm_analysis(self, metrics: TrainingMetrics) -> OptimizationSuggestion:
        """–ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ Ollama"""
        try:
            prompt = self._create_analysis_prompt(metrics)
            
            # –ó–∞–ø—Ä–æ—Å –∫ Ollama API
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'qwen2.5:latest',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º qwen2.5 –≤–º–µ—Å—Ç–æ qwen3
                    'prompt': prompt,
                    'stream': False,
                    'options': {
                        'temperature': 0.3,
                        'top_p': 0.9
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result.get('response', '')
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç –æ—Ç LLM
                try:
                    # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                    import re
                    json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
                    if json_match:
                        suggestion_data = json.loads(json_match.group())
                        
                        action_map = {
                            'continue': OptimizationAction.CONTINUE,
                            'adjust_lr': OptimizationAction.ADJUST_LR,
                            'adjust_dropout': OptimizationAction.ADJUST_DROPOUT,
                            'early_stop': OptimizationAction.EARLY_STOP,
                            'enable_gradient_clipping': OptimizationAction.ENABLE_GRADIENT_CLIPPING,
                            'adjust_weight_decay': OptimizationAction.ADJUST_WEIGHT_DECAY
                        }
                        
                        action = action_map.get(suggestion_data.get('action', 'continue'), OptimizationAction.CONTINUE)
                        
                        return OptimizationSuggestion(
                            action=action,
                            parameters=suggestion_data.get('parameters', {}),
                            confidence=suggestion_data.get('confidence', 0.7),
                            reasoning=f"ü§ñ Ollama: {suggestion_data.get('reasoning', 'LLM —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è')}"
                        )
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ Ollama: {e}")
                    print(f"–û—Ç–≤–µ—Ç: {llm_response[:200]}...")
            else:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Ollama API: {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            print(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM: {e}")
        
        # Fallback –Ω–∞ rule-based –∞–Ω–∞–ª–∏–∑
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ fallback")
        return self._rule_based_analysis(metrics)
    
    def _openai_analysis(self, metrics: TrainingMetrics) -> OptimizationSuggestion:
        """–ê–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é OpenAI API (structured output —á–µ—Ä–µ–∑ function calling)"""
        try:
            from openai import OpenAI
            
            # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key or api_key == 'your_api_key_here':
                print("‚ö†Ô∏è OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
                print("   –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–æ–±–∞–≤—å—Ç–µ –≤–∞—à —Ä–µ–∞–ª—å–Ω—ã–π API –∫–ª—é—á –≤ .env —Ñ–∞–π–ª")
                return self._rule_based_analysis(metrics)
            
            print(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI API (–∫–ª—é—á: {api_key[:8]}...)")
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
            client = OpenAI(api_key=api_key)
            
            prompt = self._create_analysis_prompt(metrics)
            
            response = client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert ML engineer specializing in hyperparameter optimization. Analyze the metrics and call the optimize_hyperparameters function with your recommendation."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                tools=[
                    {
                        "type": "function",
                        "function": {
                            "name": "optimize_hyperparameters",
                            "description": "Provide hyperparameter optimization recommendation",
                            "parameters": {
                                "type": "object",
                                "properties": {
                                    "action": {
                                        "type": "string",
                                        "enum": ["continue", "adjust_lr", "adjust_dropout", "early_stop", "enable_gradient_clipping", "adjust_weight_decay"],
                                        "description": "The optimization action to take"
                                    },
                                    "factor": {
                                        "type": "number",
                                        "description": "Learning rate adjustment factor (for adjust_lr action)"
                                    },
                                    "dropout_increase": {
                                        "type": "number",
                                        "description": "Dropout increase amount (for adjust_dropout action)"
                                    },
                                    "max_norm": {
                                        "type": "number", 
                                        "description": "Max gradient norm (for enable_gradient_clipping action)"
                                    },
                                    "confidence": {
                                        "type": "number",
                                        "minimum": 0,
                                        "maximum": 1,
                                        "description": "Confidence in the recommendation (0-1)"
                                    },
                                    "reasoning": {
                                        "type": "string",
                                        "description": "Detailed explanation for the recommendation"
                                    }
                                },
                                "required": ["action", "confidence", "reasoning"]
                            }
                        }
                    }
                ],
                tool_choice={"type": "function", "function": {"name": "optimize_hyperparameters"}},
                temperature=0.3
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ tool call
            tool_calls = response.choices[0].message.tool_calls
            if not tool_calls:
                print("‚ö†Ô∏è OpenAI –Ω–µ –≤–µ—Ä–Ω—É–ª tool call")
                return self._rule_based_analysis(metrics)
            
            # –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã —Ñ—É–Ω–∫—Ü–∏–∏
            suggestion_data = json.loads(tool_calls[0].function.arguments)
            
            action_map = {
                'continue': OptimizationAction.CONTINUE,
                'adjust_lr': OptimizationAction.ADJUST_LR,
                'adjust_dropout': OptimizationAction.ADJUST_DROPOUT,
                'early_stop': OptimizationAction.EARLY_STOP,
                'enable_gradient_clipping': OptimizationAction.ENABLE_GRADIENT_CLIPPING,
                'adjust_weight_decay': OptimizationAction.ADJUST_WEIGHT_DECAY
            }
            
            action = action_map.get(suggestion_data.get('action', 'continue'), OptimizationAction.CONTINUE)
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            parameters = {}
            if 'factor' in suggestion_data:
                parameters['factor'] = suggestion_data['factor']
                parameters['lr_factor'] = suggestion_data['factor']  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if 'dropout_increase' in suggestion_data:
                parameters['dropout_increase'] = suggestion_data['dropout_increase']
                parameters['amount'] = suggestion_data['dropout_increase']  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if 'max_norm' in suggestion_data:
                parameters['max_norm'] = suggestion_data['max_norm']
                parameters['clip_norm'] = suggestion_data['max_norm']  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                
            suggestion = OptimizationSuggestion(
                    action=action,
                    parameters=parameters,
                    confidence=suggestion_data.get('confidence', 0.8),
                    reasoning=suggestion_data.get('reasoning', '')  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤–º–µ—Å—Ç–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                )
            print(f"‚úÖ GPT-4 –ø—Ä–µ–¥–ª–æ–∂–∏–ª: {action.value} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {parameters}")
            return suggestion
                
        except ImportError:
            print("‚ö†Ô∏è –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ OpenAI API: {e}")
            import traceback
            traceback.print_exc()
        
        # Fallback –Ω–∞ rule-based –∞–Ω–∞–ª–∏–∑
        print("üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º rule-based –∞–Ω–∞–ª–∏–∑ –∫–∞–∫ fallback")
        return self._rule_based_analysis(metrics)
    
    def _create_analysis_prompt(self, metrics: TrainingMetrics) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM –∞–Ω–∞–ª–∏–∑–∞"""
        history_summary = ""
        if len(self.history) > 1:
            recent_history = self.history[-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —ç–ø–æ—Ö
            history_summary = "\nRecent training history:\n"
            for i, h in enumerate(recent_history):
                history_summary += f"Epoch {h.epoch}: train_loss={h.train_loss:.4f}, val_loss={h.val_loss:.4f}, lr={h.learning_rate:.6f}\n"
        
        prompt = f"""
You are an expert ML engineer optimizing a Transformer model for time series forecasting.

Current training metrics:
- Epoch: {metrics.epoch}
- Train Loss: {metrics.train_loss:.4f}
- Validation Loss: {metrics.val_loss:.4f}
- Learning Rate: {metrics.learning_rate:.6f}
- Loss Improvement: {metrics.loss_improvement:.4f}
- Gradient Norm: {metrics.gradient_norm:.2f}
- Training Time: {metrics.training_time:.2f}s
- Overfitting Score: {metrics.overfitting_score:.3f}
- Convergence Score: {metrics.convergence_score:.3f}

{history_summary}

Model details:
- Architecture: Transformer (encoder-decoder)
- Task: Time series forecasting
- Data: Synthetic time series with trend and seasonality
- Device: Apple M1 Max with MPS acceleration

Please analyze the training progress and suggest ONE specific action:
1. continue - if training is going well
2. adjust_lr - if learning rate needs adjustment (specify factor)
3. adjust_dropout - if overfitting detected (specify increase amount)
4. early_stop - if training should stop
5. enable_gradient_clipping - if gradients are exploding
6. adjust_weight_decay - if regularization needs adjustment

Respond in JSON format:
{{
    "action": "action_name",
    "parameters": {{"param": value}},
    "confidence": 0.8,
    "reasoning": "Detailed explanation"
}}
"""
        return prompt
    
    def apply_suggestions(self, suggestion: OptimizationSuggestion, optimizer: optim.Optimizer, 
                         model: nn.Module, gradient_clipping_state: Dict[str, Any]) -> bool:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        self.suggestions_history.append(suggestion)
        try:
            if suggestion.action == OptimizationAction.ADJUST_LR:
                factor = suggestion.parameters.get('lr_factor') or suggestion.parameters.get('factor') or 0.5
                for param_group in optimizer.param_groups:
                    param_group['lr'] *= factor
                # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è factor
                if factor > 1.0:
                    print(f"üîß LLM: –£–≤–µ–ª–∏—á–µ–Ω learning rate –Ω–∞ {factor:.2f}x ‚Üë (–Ω–æ–≤—ã–π LR: {optimizer.param_groups[0]['lr']:.6f})")
                else:
                    print(f"üîß LLM: –£–º–µ–Ω—å—à–µ–Ω learning rate –Ω–∞ {factor:.2f}x ‚Üì (–Ω–æ–≤—ã–π LR: {optimizer.param_groups[0]['lr']:.6f})")
                return True
            elif suggestion.action == OptimizationAction.ADJUST_DROPOUT:
                dropout_increase = suggestion.parameters.get('dropout_increase') or suggestion.parameters.get('amount') or 0.05
                for module in model.modules():
                    if isinstance(module, nn.Dropout):
                        module.p = min(0.5, module.p + dropout_increase)
                print(f"üîß LLM: –£–≤–µ–ª–∏—á–µ–Ω dropout –Ω–∞ {dropout_increase:.2f}")
                return True
            elif suggestion.action == OptimizationAction.ENABLE_GRADIENT_CLIPPING:
                max_norm = suggestion.parameters.get('max_norm') or suggestion.parameters.get('clip_norm') or 1.0
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ë–ê–ì–ê #1: –†–µ–∞–ª—å–Ω–æ –≤–∫–ª—é—á–∞–µ–º gradient clipping
                gradient_clipping_state['enabled'] = True
                gradient_clipping_state['max_norm'] = max_norm
                print(f"üîß LLM: –í–∫–ª—é—á–µ–Ω gradient clipping —Å max_norm={max_norm}")
                return True
            elif suggestion.action == OptimizationAction.EARLY_STOP:
                print("üõë LLM: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç early stopping")
                return False
            elif suggestion.action == OptimizationAction.CONTINUE:
                print(f"‚úÖ LLM: –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ")
                return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}")
        return True
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """–°–≤–æ–¥–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞"""
        if not self.history:
            return {}
        
        best_val_loss = min(m.val_loss for m in self.history)
        best_epoch = next(i for i, m in enumerate(self.history) if m.val_loss == best_val_loss)
        
        actions_count = {}
        for suggestion in self.suggestions_history:
            action = suggestion.action.value
            actions_count[action] = actions_count.get(action, 0) + 1
        
        return {
            'total_epochs': len(self.history),
            'best_val_loss': best_val_loss,
            'best_epoch': best_epoch,
            'final_val_loss': self.history[-1].val_loss if self.history else 0,
            'improvement': (self.history[0].val_loss - best_val_loss) / self.history[0].val_loss if self.history else 0,
            'actions_taken': actions_count,
            'avg_confidence': np.mean([s.confidence for s in self.suggestions_history]) if self.suggestions_history else 0
        }
    
    def visualize_optimization_history(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        if len(self.history) < 2:
            print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = [m.epoch for m in self.history]
        train_losses = [m.train_loss for m in self.history]
        val_losses = [m.val_loss for m in self.history]
        learning_rates = [m.learning_rate for m in self.history]
        overfitting_scores = [m.overfitting_score for m in self.history]
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        axes[0, 0].plot(epochs, train_losses, label='Train Loss', alpha=0.7)
        axes[0, 0].plot(epochs, val_losses, label='Val Loss', alpha=0.7)
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training Progress with LLM Optimization')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ learning rate
        axes[0, 1].plot(epochs, learning_rates, color='orange', marker='o', markersize=3)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Learning Rate')
        axes[0, 1].set_title('LLM Learning Rate Adjustments')
        axes[0, 1].set_yscale('log')
        axes[0, 1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        axes[1, 0].plot(epochs, overfitting_scores, color='red', alpha=0.7)
        axes[1, 0].axhline(y=self.rules_config['overfitting_threshold'], color='red', linestyle='--', alpha=0.5)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Overfitting Score')
        axes[1, 0].set_title('Overfitting Detection')
        axes[1, 0].grid(True, alpha=0.3)
        
        # –î–µ–π—Å—Ç–≤–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        actions_count = {}
        for suggestion in self.suggestions_history:
            action = suggestion.action.value
            actions_count[action] = actions_count.get(action, 0) + 1
        
        if actions_count:
            actions = list(actions_count.keys())
            counts = list(actions_count.values())
            axes[1, 1].bar(actions, counts, alpha=0.7)
            axes[1, 1].set_xlabel('Action')
            axes[1, 1].set_ylabel('Count')
            axes[1, 1].set_title('LLM Actions Taken')
            axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()

def train_model_with_llm(model: nn.Module, 
                        train_loader: DataLoader, 
                        val_loader: DataLoader,
                        n_epochs: int = 100,
                        learning_rate: float = 0.001,
                        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                        llm_optimizer_type: str = "rule_based",
                        llm_every_n_epochs: int = 3) -> Tuple[List[float], List[float], LLMHyperparameterOptimizer]:
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –±–∞–≥–∞–º–∏)"""
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)
    llm_optimizer = LLMHyperparameterOptimizer(model_type=llm_optimizer_type)
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 15
    best_model_state = None
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ë–ê–ì–ê #1: –°–æ—Å—Ç–æ—è–Ω–∏–µ gradient clipping –≤ —Å–ª–æ–≤–∞—Ä–µ
    gradient_clipping_state = {'enabled': False, 'max_norm': 1.0}
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π ({llm_optimizer_type})")
    print(f"   üõë Early stopping: patience={patience}")
    training_start_time = time.time()
    no_improve_epochs = 0
    for epoch in range(n_epochs):
        epoch_start_time = time.time()
        model.train()
        train_loss = 0
        total_grad_norm = 0
        train_start_time = time.time()
        for batch_idx, (src, tgt) in enumerate(train_loader):
            src, tgt = src.to(device), tgt.to(device)
            tgt_input = torch.cat([src[:, -1:, :], tgt[:, :-1, :]], dim=1)
            tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)
            optimizer.zero_grad()
            output = model(src, tgt_input, tgt_mask=tgt_mask)
            loss = criterion(output, tgt)
            loss.backward()
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ë–ê–ì–ê #1: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π gradient clipping
            if gradient_clipping_state['enabled']:
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping_state['max_norm'])
            else:
                # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º—É –±–µ–∑ –∫–ª–∏–ø–ø–∏–Ω–≥–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                grad_norm = torch.norm(torch.stack([
                    p.grad.detach().norm(2) for p in model.parameters() if p.grad is not None
                ]), 2).item()
            
            total_grad_norm += grad_norm
            optimizer.step()
            train_loss += loss.item()
        train_time = time.time() - train_start_time
        avg_grad_norm = total_grad_norm / len(train_loader)
        model.eval()
        val_loss = 0
        val_start_time = time.time()
        with torch.no_grad():
            for src, tgt in val_loader:
                src, tgt = src.to(device), tgt.to(device)
                tgt_input = torch.cat([src[:, -1:, :], tgt[:, :-1, :]], dim=1)
                tgt_mask = model.generate_square_subsequent_mask(tgt_input.size(1)).to(device)
                output = model(src, tgt_input, tgt_mask=tgt_mask)
                loss = criterion(output, tgt)
                val_loss += loss.item()
        val_time = time.time() - val_start_time
        epoch_time = time.time() - epoch_start_time
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        loss_improvement = (val_losses[-2] - avg_val_loss) if len(val_losses) > 1 else 0.0
        metrics = TrainingMetrics(
            epoch=epoch,
            train_loss=avg_train_loss,
            val_loss=avg_val_loss,
            learning_rate=optimizer.param_groups[0]['lr'],
            loss_improvement=loss_improvement,
            gradient_norm=avg_grad_norm,
            training_time=epoch_time
        )
        force_trigger = no_improve_epochs >= 5
        if epoch % llm_every_n_epochs == 0 or force_trigger:
            suggestion = llm_optimizer.analyze_training_progress(metrics)
            should_continue = llm_optimizer.apply_suggestions(suggestion, optimizer, model, gradient_clipping_state)
            print(f"[LLM] epoch={epoch} action={suggestion.action.value} params={suggestion.parameters} val_loss={avg_val_loss:.4f}")
        else:
            should_continue = True
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ë–ê–ì–ê #3: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ no_improve_epochs
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            no_improve_epochs = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–¥–µ—Å—å
        else:
            patience_counter += 1
            no_improve_epochs += 1
        if epoch % 5 == 0:
            print(f'Epoch {epoch:3d}/{n_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print(f'   ‚è±Ô∏è  Train: {train_time:.2f}s | Val: {val_time:.2f}s | Total: {epoch_time:.2f}s | Speed: {len(train_loader)/train_time:.1f} batch/s')
            print(f'   üéØ Best Val Loss: {best_val_loss:.4f} | Patience: {patience_counter}/{patience}')
        if not should_continue or patience_counter >= patience:
            if not should_continue:
                print(f'\nüõë LLM —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}!')
            else:
                print(f'\nüõë Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch}! Val loss –Ω–µ —É–ª—É—á—à–∞–ª—Å—è {patience} —ç–ø–æ—Ö.')
            break
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å Val Loss: {best_val_loss:.6f}")
    total_training_time = time.time() - training_start_time
    print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_training_time:.1f}s ({total_training_time/60:.1f} –º–∏–Ω)")
    print(f"   –í—Ä–µ–º—è –Ω–∞ —ç–ø–æ—Ö—É: {total_training_time/len(train_losses):.1f}s")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {train_losses[-1]:.6f}")
    print(f"   –õ—É—á—à–∏–π Val Loss: {best_val_loss:.6f}")
    performance = llm_optimizer.get_performance_summary()
    print(f"\nüß† LLM –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:")
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {performance.get('improvement', 0)*100:.1f}%")
    print(f"   –î–µ–π—Å—Ç–≤–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {sum(performance.get('actions_taken', {}).values())}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {performance.get('avg_confidence', 0):.2f}")
    actions_count = performance.get('actions_taken', {})
    if actions_count:
        print("   –¢–æ–ø-5 –¥–µ–π—Å—Ç–≤–∏–π LLM:")
        for i, (action, count) in enumerate(sorted(actions_count.items(), key=lambda x: -x[1])[:5]):
            print(f"      {i+1}. {action}: {count} —Ä–∞–∑")
    return train_losses, val_losses, llm_optimizer

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    script_start_time = time.time()
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    seq_length = 60
    pred_length = 15
    batch_size = 32
    n_epochs = 30   # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è OpenAI
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    data_start_time = time.time()
    data = create_synthetic_data(2000)
    data_prep_time = time.time() - data_start_time
    print(f"   ‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã –∑–∞ {data_prep_time:.2f}s")
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    train_size = int(0.8 * len(data))
    train_data = data[:train_size]
    test_data = data[train_size:]
    
    mean = train_data.mean()
    std = train_data.std()
    train_data_norm = (train_data - mean) / std
    test_data_norm = (test_data - mean) / std
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_dataset = TimeSeriesDataset(train_data_norm[:-100], seq_length, pred_length, augment=False)
    val_dataset = TimeSeriesDataset(train_data_norm[-100-seq_length-pred_length:], seq_length, pred_length, augment=False)
    
    # DataLoader –¥–ª—è MPS: num_workers=0, pin_memory=False, persistent_workers=False
    if torch.backends.mps.is_available():
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False, persistent_workers=False)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0, pin_memory=False, persistent_workers=False)
    else:
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, pin_memory=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    if torch.backends.mps.is_available():
        device = 'mps'
        print("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º Metal Performance Shaders (MPS) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è!")
        print(f"   MPS built: {torch.backends.mps.is_built()}")
        if hasattr(torch.mps, 'empty_cache'):
            torch.mps.empty_cache()
    elif torch.cuda.is_available():
        device = 'cuda'
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è!")
    else:
        device = 'cpu'
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞...")
    model_start_time = time.time()
    model = TimeSeriesTransformer(
        input_dim=1,
        d_model=128,
        n_heads=8,
        n_encoder_layers=4,
        n_decoder_layers=4,
        d_ff=512,
        max_seq_length=200,
        dropout=0.2
    )
    
    model_creation_time = time.time() - model_start_time
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∑–∞ {model_creation_time:.2f}s")
    print(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   üìà Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}")
    print(f"   ‚öôÔ∏è  Batch size: {batch_size} | Epochs: {n_epochs}")
    
    # –û–±—É—á–µ–Ω–∏–µ —Å LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    print("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π...")
    train_phase_start = time.time()
    train_losses, val_losses, llm_optimizer = train_model_with_llm(
        model, train_loader, val_loader, 
        n_epochs=n_epochs, 
        learning_rate=0.001, 
        device=device,
        llm_optimizer_type="openai",  # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å OpenAI!
        llm_every_n_epochs=3
    )
    train_phase_time = time.time() - train_phase_start
    
    # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüîÆ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
    print(f"   üìè –î–ª–∏–Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_data_norm)}")
    prediction_start_time = time.time()
    model.eval()
    test_predictions = []
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    prediction_steps = 0
    for i in range(0, len(test_data_norm) - seq_length, pred_length):
        src = torch.FloatTensor(test_data_norm[i:i+seq_length]).unsqueeze(0)
        pred = forecast(model, src, pred_length, device, temperature=1.0)
        test_predictions.extend(pred.squeeze().cpu().numpy())
        prediction_steps += 1
        
        if prediction_steps % 5 == 0:
            print(f"   üîÑ –í—ã–ø–æ–ª–Ω–µ–Ω–æ {prediction_steps} —à–∞–≥–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
        
        if i + seq_length + pred_length >= len(test_data_norm):
            break
    
    prediction_time = time.time() - prediction_start_time
    print(f"   ‚úÖ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {prediction_time:.2f}s")
    print(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {prediction_steps} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(test_predictions)}")
    print(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {prediction_steps/prediction_time:.1f} —à–∞–≥–æ–≤/—Å–µ–∫")
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    test_predictions = np.array(test_predictions) * std + mean
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
    print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è...")
    viz_start_time = time.time()
    visualize_results(
        train_data.squeeze(), 
        test_data[:len(test_predictions)].squeeze(),
        test_predictions.squeeze(),
        train_losses,
        val_losses
    )
    viz_time = time.time() - viz_start_time
    print(f"   ‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∑–∞ {viz_time:.2f}s")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    actual = test_data[:len(test_predictions)].squeeze()
    predicted = test_predictions.squeeze()
    mse = np.mean((actual - predicted) ** 2)
    mae = np.mean(np.abs(actual - predicted))
    
    print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"   MSE: {mse:.4f}")
    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {np.sqrt(mse):.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    print("\nüìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è LLM –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
    llm_optimizer.visualize_optimization_history()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'test_predictions': test_predictions.tolist() if isinstance(test_predictions, np.ndarray) else test_predictions,
        'metrics': {
            'mse': float(mse),
            'mae': float(mae),
            'rmse': float(np.sqrt(mse))
        },
        'llm_performance': llm_optimizer.get_performance_summary(),
        'model_config': {
            'input_dim': 1,
            'd_model': 128,
            'n_heads': 8,
            'n_encoder_layers': 4,
            'n_decoder_layers': 4,
            'd_ff': 512,
            'max_seq_length': 200,
            'dropout': 0.2
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_start_time = time.time()
    torch.save({
        'model_state_dict': model.state_dict(),
        'mean': mean,
        'std': std,
        'model_config': results['model_config'],
        'llm_performance': results['llm_performance']
    }, 'llm_transformer_model.pth')
    save_time = time.time() - save_start_time
    
    with open('llm_optimization_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∑–∞ {save_time:.2f}s –≤ 'llm_transformer_model.pth'")
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'llm_optimization_results.json'")
    
    total_script_time = time.time() - script_start_time
    print(f"\nüéØ –û–ë–©–ï–ï –í–†–ï–ú–Ø –í–´–ü–û–õ–ù–ï–ù–ò–Ø: {total_script_time:.1f}s ({total_script_time/60:.1f} –º–∏–Ω)")
    print(f"   üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {data_prep_time:.2f}s")
    print(f"   üß† –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_creation_time:.2f}s") 
    print(f"   ü§ñ LLM –æ–±—É—á–µ–Ω–∏–µ: {train_phase_time:.1f}s")
    print(f"   üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ: {prediction_time:.2f}s")
    print(f"   üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: {viz_time:.2f}s")

if __name__ == "__main__":
    main()